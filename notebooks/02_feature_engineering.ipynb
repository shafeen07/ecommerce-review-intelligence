{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349a8287-4047-4ef4-9648-8f7336e4670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Set Java home\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.28.6-hotspot\"\n",
    "os.environ['PATH'] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.28.6-hotspot\\bin\" + os.pathsep + os.environ['PATH']\n",
    "\n",
    "# Initialize findspark\n",
    "findspark.init()\n",
    "\n",
    "print(\"✓ Environment configured\")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, min, max, stddev, \n",
    "    year, month, dayofweek, length, \n",
    "    when, expr, approx_count_distinct,\n",
    "    percentile_approx, to_timestamp, from_unixtime\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133e3b59-c905-402b-a15e-cf294831ccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark Session Created\n",
      "  Version: 3.5.3\n",
      "  Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. INITIALIZE SPARK SESSION\n",
    "# ============================================================================\n",
    "\n",
    "# Configure for local development (adjust memory based on your system)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceReviewIntelligence\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark Session Created\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7b84c0-8892-45aa-a45e-c9d15b512950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Loading data from: C:\\Users\\shafe\\OneDrive\\Desktop\\ecommerce-intelligence\\data\\raw\\electronics_sample_2M.jsonl.gz\n",
      "✓ Data loaded successfully\n",
      "  Partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Path to your sample\n",
    "data_path = r\"C:\\Users\\shafe\\OneDrive\\Desktop\\ecommerce-intelligence\\data\\raw\\electronics_sample_2M.jsonl.gz\"\n",
    "\n",
    "print(f\"\\n📂 Loading data from: {data_path}\")\n",
    "\n",
    "# Load compressed JSONL - Spark handles .gz automatically\n",
    "df = spark.read.json(data_path)\n",
    "\n",
    "print(f\"✓ Data loaded successfully\")\n",
    "print(f\"  Partitions: {df.rdd.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccfde52-7896-4f4f-90e8-ca814305a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCHEMA OVERVIEW\n",
      "================================================================================\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Total Rows: 2,051,569\n",
      "Total Columns: 10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. SCHEMA INSPECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCHEMA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal Rows: {df.count():,}\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f2f962-b3a0-42ba-85f9-a93755610a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLUMN INVENTORY\n",
      "================================================================================\n",
      "Available columns: ['asin', 'helpful_vote', 'images', 'parent_asin', 'rating', 'text', 'timestamp', 'title', 'user_id', 'verified_purchase']\n",
      "\n",
      "Column Status:\n",
      "  ✓ rating               → Sentiment trends\n",
      "  ✓ text                 → Theme extraction\n",
      "  ✓ title                → Additional text analysis\n",
      "  ✓ timestamp            → Temporal analysis\n",
      "  ✓ parent_asin          → Product identification\n",
      "  ✓ asin                 → Review/product linking\n",
      "  ✓ verified_purchase    → Quality filtering\n",
      "  ✓ helpful_vote         → Review quality indicator\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. COLUMN AVAILABILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COLUMN INVENTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check what columns actually exist in your data\n",
    "available_columns = df.columns\n",
    "print(f\"Available columns: {available_columns}\\n\")\n",
    "\n",
    "# Key columns for your 5 features\n",
    "required_cols = {\n",
    "    'rating': 'Sentiment trends',\n",
    "    'text': 'Theme extraction', \n",
    "    'title': 'Additional text analysis',\n",
    "    'timestamp': 'Temporal analysis',\n",
    "    'parent_asin': 'Product identification',\n",
    "    'asin': 'Review/product linking',\n",
    "    'verified_purchase': 'Quality filtering',\n",
    "    'helpful_vote': 'Review quality indicator'\n",
    "}\n",
    "\n",
    "print(\"Column Status:\")\n",
    "for col_name, purpose in required_cols.items():\n",
    "    status = \"✓\" if col_name in available_columns else \"✗\"\n",
    "    print(f\"  {status} {col_name:20s} → {purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99be5f09-d400-40a3-aa61-54c4bfb1c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "Column Completeness (% non-null):\n",
      "  asin                : 100.00% (2,051,569 / 2,051,569)\n",
      "  helpful_vote        : 100.00% (2,051,569 / 2,051,569)\n",
      "  images              : 100.00% (2,051,569 / 2,051,569)\n",
      "  parent_asin         : 100.00% (2,051,569 / 2,051,569)\n",
      "  rating              : 100.00% (2,051,569 / 2,051,569)\n",
      "  text                : 100.00% (2,051,569 / 2,051,569)\n",
      "  timestamp           : 100.00% (2,051,569 / 2,051,569)\n",
      "  title               : 100.00% (2,051,569 / 2,051,569)\n",
      "  user_id             : 100.00% (2,051,569 / 2,051,569)\n",
      "  verified_purchase   : 100.00% (2,051,569 / 2,051,569)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. DATA QUALITY OVERVIEW\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate completeness for each column\n",
    "print(\"\\nColumn Completeness (% non-null):\")\n",
    "total_rows = df.count()\n",
    "\n",
    "for col_name in df.columns:\n",
    "    non_null_count = df.filter(col(col_name).isNotNull()).count()\n",
    "    completeness = (non_null_count / total_rows) * 100\n",
    "    print(f\"  {col_name:20s}: {completeness:6.2f}% ({non_null_count:,} / {total_rows:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "561a1b24-d677-4f0d-b5ee-53e1d695c708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RATING DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Rating Breakdown:\n",
      "  1.0 ⭐: 161,497.0 ( 7.87%) ███\n",
      "  2.0 ⭐: 84,509.0 ( 4.12%) ██\n",
      "  3.0 ⭐: 124,124.0 ( 6.05%) ███\n",
      "  4.0 ⭐: 249,137.0 (12.14%) ██████\n",
      "  5.0 ⭐: 1,432,302.0 (69.81%) ██████████████████████████████████\n",
      "\n",
      "Overall Average Rating: 4.32 ⭐\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. RATING DISTRIBUTION (Feature 1: Sentiment Trends)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RATING DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rating_dist = df.groupBy(\"rating\") \\\n",
    "    .agg(count(\"*\").alias(\"review_count\")) \\\n",
    "    .orderBy(\"rating\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"\\nRating Breakdown:\")\n",
    "for _, row in rating_dist.iterrows():\n",
    "    pct = (row['review_count'] / total_rows) * 100\n",
    "    bar = \"█\" * int(pct / 2)\n",
    "    print(f\"  {row['rating']:.1f} ⭐: {row['review_count']:8,} ({pct:5.2f}%) {bar}\")\n",
    "\n",
    "# Calculate average rating\n",
    "avg_rating = df.agg(avg(\"rating\").alias(\"avg_rating\")).collect()[0]['avg_rating']\n",
    "print(f\"\\nOverall Average Rating: {avg_rating:.2f} ⭐\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27045c0-8774-4a80-8585-074399edb576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEXT QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Review Text Length Statistics (characters):\n",
      "  Total Reviews:  2,051,569\n",
      "  Average:        173.2\n",
      "  Median:         87\n",
      "  Std Dev:        292.3\n",
      "  Min:            0\n",
      "  Max:            22,465\n",
      "  25th %ile:      35\n",
      "  75th %ile:      200\n",
      "\n",
      "Text Length Distribution:\n",
      "+-----------------+------+\n",
      "|text_category    |count |\n",
      "+-----------------+------+\n",
      "|Short (50-200)   |847789|\n",
      "|Very Short (<50) |689991|\n",
      "|Medium (200-500) |370025|\n",
      "|Long (500-1000)  |106798|\n",
      "|Very Long (1000+)|36966 |\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. TEXT QUALITY ANALYSIS (Feature 2: Theme Extraction)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add text length column\n",
    "df_with_length = df.withColumn(\"text_length\", length(col(\"text\")))\n",
    "\n",
    "# Text length statistics\n",
    "text_stats = df_with_length.select(\n",
    "    count(\"text\").alias(\"total_reviews\"),\n",
    "    avg(\"text_length\").alias(\"avg_length\"),\n",
    "    min(\"text_length\").alias(\"min_length\"),\n",
    "    max(\"text_length\").alias(\"max_length\"),\n",
    "    stddev(\"text_length\").alias(\"std_length\"),\n",
    "    percentile_approx(\"text_length\", 0.5).alias(\"median_length\"),\n",
    "    percentile_approx(\"text_length\", 0.25).alias(\"p25_length\"),\n",
    "    percentile_approx(\"text_length\", 0.75).alias(\"p75_length\")\n",
    ").toPandas()\n",
    "\n",
    "print(\"\\nReview Text Length Statistics (characters):\")\n",
    "print(f\"  Total Reviews:  {text_stats['total_reviews'][0]:,}\")\n",
    "print(f\"  Average:        {text_stats['avg_length'][0]:.1f}\")\n",
    "print(f\"  Median:         {text_stats['median_length'][0]:.0f}\")\n",
    "print(f\"  Std Dev:        {text_stats['std_length'][0]:.1f}\")\n",
    "print(f\"  Min:            {text_stats['min_length'][0]}\")\n",
    "print(f\"  Max:            {text_stats['max_length'][0]:,}\")\n",
    "print(f\"  25th %ile:      {text_stats['p25_length'][0]:.0f}\")\n",
    "print(f\"  75th %ile:      {text_stats['p75_length'][0]:.0f}\")\n",
    "\n",
    "# Text quality categories\n",
    "text_quality = df_with_length.withColumn(\n",
    "    \"text_category\",\n",
    "    when(col(\"text_length\") < 50, \"Very Short (<50)\")\n",
    "    .when(col(\"text_length\") < 200, \"Short (50-200)\")\n",
    "    .when(col(\"text_length\") < 500, \"Medium (200-500)\")\n",
    "    .when(col(\"text_length\") < 1000, \"Long (500-1000)\")\n",
    "    .otherwise(\"Very Long (1000+)\")\n",
    ")\n",
    "\n",
    "print(\"\\nText Length Distribution:\")\n",
    "text_quality.groupBy(\"text_category\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d65e5be4-521f-4084-b828-2893686e8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEMPORAL DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Reviews by Year:\n",
      "+-----------+------+\n",
      "|review_year| count|\n",
      "+-----------+------+\n",
      "|       2007|   602|\n",
      "|       2008|  4249|\n",
      "|       2009|   561|\n",
      "|       2010|   775|\n",
      "|       2011|  1482|\n",
      "|       2012|  3595|\n",
      "|       2013| 10041|\n",
      "|       2014| 28903|\n",
      "|       2015| 70234|\n",
      "|       2016|184147|\n",
      "|       2017|291110|\n",
      "|       2018|288758|\n",
      "|       2019|338299|\n",
      "|       2020|404252|\n",
      "|       2021|275646|\n",
      "|       2022|125247|\n",
      "|       2023| 23668|\n",
      "+-----------+------+\n",
      "\n",
      "\n",
      "Date Range:\n",
      "  Earliest: 2007-11-19 00:16:47\n",
      "  Latest:   2023-09-10 18:05:53\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. TEMPORAL ANALYSIS (Feature 5: Review Velocity)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEMPORAL DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert timestamp to date (assuming Unix timestamp in milliseconds)\n",
    "df_temporal = df.withColumn(\n",
    "    \"review_date\",\n",
    "    to_timestamp(from_unixtime(col(\"timestamp\") / 1000))\n",
    ").withColumn(\n",
    "    \"review_year\",\n",
    "    year(\"review_date\")\n",
    ").withColumn(\n",
    "    \"review_month\",\n",
    "    month(\"review_date\")\n",
    ")\n",
    "\n",
    "# Year distribution\n",
    "print(\"\\nReviews by Year:\")\n",
    "df_temporal.groupBy(\"review_year\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"review_year\") \\\n",
    "    .show(20)\n",
    "\n",
    "# Date range\n",
    "date_range = df_temporal.select(\n",
    "    min(\"review_date\").alias(\"earliest\"),\n",
    "    max(\"review_date\").alias(\"latest\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nDate Range:\")\n",
    "print(f\"  Earliest: {date_range['earliest']}\")\n",
    "print(f\"  Latest:   {date_range['latest']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd75bbc-70ae-454c-97b9-cff6c2246d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRODUCT-LEVEL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 20 Products by Review Volume:\n",
      "+-----------+------------+------------------+------------------+\n",
      "|parent_asin|review_count|avg_rating        |rating_stddev     |\n",
      "+-----------+------------+------------------+------------------+\n",
      "|B075X8471B |178239      |4.392057854902687 |1.183661113282277 |\n",
      "|B07GZFM1ZM |140751      |4.511754801031609 |1.0944273559836395|\n",
      "|B01K8B8YA8 |119051      |4.346901747990357 |1.1632273628160226|\n",
      "|B010BWYDYA |103964      |4.143261128852295 |1.2850597477632528|\n",
      "|B07H65KP63 |95397       |4.626235625858255 |0.9293519769296041|\n",
      "|B0791TX5P5 |88798       |4.449582197797247 |1.1758164076342243|\n",
      "|B08XPWDSWW |72566       |4.260438772979081 |1.2705146616240275|\n",
      "|B07S764D9V |55743       |4.255404266006494 |1.238233047702127 |\n",
      "|B07KTYJ769 |51867       |4.575818921472227 |1.0733212524663065|\n",
      "|B0BW4PFM58 |51568       |4.3570819112627985|1.2027619433283017|\n",
      "|B07HZLHPKP |46254       |4.400614000951269 |1.139376102664279 |\n",
      "|B07456BG8N |44813       |4.505032021957914 |1.0414950913610275|\n",
      "|B07PHQ93TV |43892       |4.356625353139524 |1.1780957815416158|\n",
      "|B08XNCHTCY |43463       |4.289418585923659 |1.2486466246522279|\n",
      "|B07N8VFFNS |41935       |3.9371169667342314|1.4513557048538162|\n",
      "|B08F1P3BCC |41696       |4.373369148119724 |1.1980856647806246|\n",
      "|B07P374FF3 |40134       |3.9020282055115363|1.4697404251607136|\n",
      "|B01MTF2Z37 |39458       |4.098864615540575 |1.3620079830418006|\n",
      "|B00OQVZDJM |39114       |4.464769647696477 |1.0483657316497268|\n",
      "|B07VXXBTX4 |33869       |4.527414449791845 |1.0367892562296537|\n",
      "+-----------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Product Summary:\n",
      "  Unique Products: 49\n",
      "  Avg Reviews/Product: 41869\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. PRODUCT ANALYSIS (Feature 3: Competitive Analysis)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCT-LEVEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Top products by review count\n",
    "print(\"\\nTop 20 Products by Review Volume:\")\n",
    "product_stats = df.groupBy(\"parent_asin\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"review_count\"),\n",
    "        avg(\"rating\").alias(\"avg_rating\"),\n",
    "        stddev(\"rating\").alias(\"rating_stddev\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"review_count\").desc())\n",
    "\n",
    "product_stats.show(20, truncate=False)\n",
    "\n",
    "# Product distribution summary\n",
    "product_summary = df.agg(\n",
    "    approx_count_distinct(\"parent_asin\").alias(\"unique_products\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nProduct Summary:\")\n",
    "print(f\"  Unique Products: {product_summary['unique_products']:,}\")\n",
    "print(f\"  Avg Reviews/Product: {total_rows / product_summary['unique_products']:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd035c9-5096-4cee-8402-7d4b7df84fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REVIEW QUALITY INDICATORS\n",
      "================================================================================\n",
      "\n",
      "Verified Purchase Distribution:\n",
      "  True : 1,976,792 (96.36%)\n",
      "  False:   74,777 ( 3.64%)\n",
      "\n",
      "Helpfulness Voting:\n",
      "  Reviews with votes: 172,585\n",
      "  Avg helpful votes:  0.69\n",
      "  Max helpful votes:  46,841\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. VERIFICATION & HELPFULNESS (Quality Indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REVIEW QUALITY INDICATORS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'verified_purchase' in df.columns:\n",
    "    verified_dist = df.groupBy(\"verified_purchase\") \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    print(\"\\nVerified Purchase Distribution:\")\n",
    "    for _, row in verified_dist.iterrows():\n",
    "        pct = (row['count'] / total_rows) * 100\n",
    "        print(f\"  {str(row['verified_purchase']):5s}: {row['count']:8,} ({pct:5.2f}%)\")\n",
    "\n",
    "if 'helpful_vote' in df.columns:\n",
    "    helpful_stats = df.select(\n",
    "        count(when(col(\"helpful_vote\") > 0, 1)).alias(\"reviews_with_votes\"),\n",
    "        avg(\"helpful_vote\").alias(\"avg_helpful\"),\n",
    "        max(\"helpful_vote\").alias(\"max_helpful\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"\\nHelpfulness Voting:\")\n",
    "    print(f\"  Reviews with votes: {helpful_stats['reviews_with_votes']:,}\")\n",
    "    print(f\"  Avg helpful votes:  {helpful_stats['avg_helpful']:.2f}\")\n",
    "    print(f\"  Max helpful votes:  {helpful_stats['max_helpful']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9acd7f-809f-4622-83b4-f96824980e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE RECORDS\n",
      "================================================================================\n",
      "\n",
      "Random Sample of Reviews:\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------\n",
      " rating            | 3.0                                                                                                  \n",
      " title             | Good system, but with significant limitations                                                        \n",
      " text              | this is a very solid home camera system. I would recommend it to anyone looking to improve their ... \n",
      " verified_purchase | true                                                                                                 \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------\n",
      " rating            | 5.0                                                                                                  \n",
      " title             | Glad I bought.  Very happy with these.                                                               \n",
      " text              | Nice clear sound and pairs and connects super easy.  These easily will fit in a pocket or the pal... \n",
      " verified_purchase | true                                                                                                 \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------\n",
      " rating            | 2.0                                                                                                  \n",
      " title             | Great idea and concept, serious execution issues                                                     \n",
      " text              | This is probably the best doorbell idea available at the time of writing. It offers great capabil... \n",
      " verified_purchase | false                                                                                                \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------\n",
      " rating            | 5.0                                                                                                  \n",
      " title             | Always a good product                                                                                \n",
      " text              | Good product for streaming                                                                           \n",
      " verified_purchase | true                                                                                                 \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------\n",
      " rating            | 4.0                                                                                                  \n",
      " title             | Good purchase, small glitch                                                                          \n",
      " text              | I purchased this for my daughter for Christmas. Easy set up except the back of the remote is VERY... \n",
      " verified_purchase | true                                                                                                 \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 11. SAMPLE RECORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE RECORDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRandom Sample of Reviews:\")\n",
    "df.select(\"rating\", \"title\", \"text\", \"verified_purchase\") \\\n",
    "    .sample(fraction=0.0001, seed=42) \\\n",
    "    .show(5, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07fffc4a-b4bf-4715-b5f0-251af66a93c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INSIGHTS FOR YOUR 5 FEATURES\n",
      "================================================================================\n",
      "\n",
      "✓ FEATURE 1: SENTIMENT TRENDS OVER TIME\n",
      "  → Rich temporal data spanning multiple years\n",
      "  → Rating distribution shows clear patterns\n",
      "  → Can track sentiment shifts by product/time period\n",
      "  \n",
      "✓ FEATURE 2: THEME EXTRACTION  \n",
      "  → Text lengths vary significantly (good for NLP)\n",
      "  → Most reviews are substantive (>50 chars)\n",
      "  → Can use PySpark ML for topic modeling or AWS Bedrock for theme extraction\n",
      "  \n",
      "✓ FEATURE 3: COMPETITIVE PRODUCT ANALYSIS\n",
      "  → 47 high-density products perfect for comparison\n",
      "  → Mix of product categories (audio, smart home, Amazon devices)\n",
      "  → Can compare ratings, themes, velocity across competitors\n",
      "  \n",
      "✓ FEATURE 4: PRODUCT SUCCESS PREDICTION\n",
      "  → Can use rating trends, review velocity, sentiment as features\n",
      "  → Verified purchase flag helps with quality\n",
      "  → Helpful votes indicate review impact\n",
      "  \n",
      "✓ FEATURE 5: REVIEW VELOCITY ANALYSIS\n",
      "  → Complete timestamp data for time-series analysis\n",
      "  → Can detect review spikes, seasonal patterns\n",
      "  → Velocity changes may predict product lifecycle stage\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Build PySpark transformation pipeline for each feature\n",
      "2. Design Redshift schema for aggregated insights\n",
      "3. Integrate AWS Bedrock for RAG-based natural language queries\n",
      "4. Create sample visualizations/dashboard\n",
      "5. Deploy to EMR for production-scale processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 12. KEY INSIGHTS FOR FEATURE DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHTS FOR YOUR 5 FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "✓ FEATURE 1: SENTIMENT TRENDS OVER TIME\n",
    "  → Rich temporal data spanning multiple years\n",
    "  → Rating distribution shows clear patterns\n",
    "  → Can track sentiment shifts by product/time period\n",
    "  \n",
    "✓ FEATURE 2: THEME EXTRACTION  \n",
    "  → Text lengths vary significantly (good for NLP)\n",
    "  → Most reviews are substantive (>50 chars)\n",
    "  → Can use PySpark ML for topic modeling or AWS Bedrock for theme extraction\n",
    "  \n",
    "✓ FEATURE 3: COMPETITIVE PRODUCT ANALYSIS\n",
    "  → 47 high-density products perfect for comparison\n",
    "  → Mix of product categories (audio, smart home, Amazon devices)\n",
    "  → Can compare ratings, themes, velocity across competitors\n",
    "  \n",
    "✓ FEATURE 4: PRODUCT SUCCESS PREDICTION\n",
    "  → Can use rating trends, review velocity, sentiment as features\n",
    "  → Verified purchase flag helps with quality\n",
    "  → Helpful votes indicate review impact\n",
    "  \n",
    "✓ FEATURE 5: REVIEW VELOCITY ANALYSIS\n",
    "  → Complete timestamp data for time-series analysis\n",
    "  → Can detect review spikes, seasonal patterns\n",
    "  → Velocity changes may predict product lifecycle stage\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Build PySpark transformation pipeline for each feature\n",
    "2. Design Redshift schema for aggregated insights\n",
    "3. Integrate AWS Bedrock for RAG-based natural language queries\n",
    "4. Create sample visualizations/dashboard\n",
    "5. Deploy to EMR for production-scale processing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93800ac-25e7-4604-86a1-d3837f5359b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DataFrame cached for subsequent analysis\n",
      "\n",
      "🎯 Ready to build transformation pipeline!\n",
      "\n",
      "📊 Exploration Complete!\n",
      "   Total Reviews: 2,051,569\n",
      "   Ready for feature engineering in next notebook!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 13. SAVE EXPLORATION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Cache the DataFrame for faster subsequent operations\n",
    "df.cache()\n",
    "\n",
    "print(\"\\n✓ DataFrame cached for subsequent analysis\")\n",
    "print(\"\\n🎯 Ready to build transformation pipeline!\")\n",
    "\n",
    "# Optional: Save exploration stats to file\n",
    "exploration_stats = {\n",
    "    'total_rows': total_rows,\n",
    "    'unique_products': product_summary['unique_products'],\n",
    "    'avg_rating': avg_rating,\n",
    "    'date_range': (str(date_range['earliest']), str(date_range['latest']))\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 Exploration Complete!\")\n",
    "print(f\"   Total Reviews: {total_rows:,}\")\n",
    "print(f\"   Ready for feature engineering in next notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3771496-3cc8-4bc3-86e2-308e2dfc1a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
